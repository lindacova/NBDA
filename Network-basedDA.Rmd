---
title: "Network-based Data Analysis"
output:
  github_document:
    toc: true
    toc_depth: 2
    # html_document:
    #   df_print: paged
    #   toc: yes
    #   theme: yeti
---

```{r include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r packages, warning=FALSE, results='hide', message=FALSE}
my_colors <- c("#2e005d", "#5c008b", "#8e008b", "#ff8300", "#ff6200", "#d1105a", "#05a8aa")
library("recount3")
library("ggplot2")
library("factoextra")
library("DESeq2")
library("useful")
library("pheatmap")
library("viridis")
library("ALL")
library("genefilter")
library("randomForest")
library("glmnet")
library("ggtree")
library("rScudo")
library("caret")
library("igraph")
library("RColorBrewer")
```

# Dataset selection (week 1)

__Recount__ --> They performed all the steps on dataasets from GEO, starting from the sequencing output. Recount directly provides raw counts: a _count matrix_ (row=genes, columns=samples)

Our project starts from count matrices.

Usually control vs test:

* Placebo vs Drug treatment
* Wild-type vs Knockout
* Healthy vs Patient
* Normal tissue vs Cancerous tissue
* Time = 0 vs Time = 1 Time = 2â€¦

Biological replicates --> necessary: independent biological samples
Technical replicates --> not necessary for us: same samples on different microarrays

Depending on the biological questions, there are various types of knowledge that can be extracted by a set of differentially expressed genes. Possible questions:

__Class discovery__ --> Identify groups of genes having a similar expression profile

__Class Prediction__ --> Find a rule useful for classifying samples. For example for diagnosis based on expression profiles


```{r load_dataset warning=FALSE, message=FALSE}

## Recount 3 korean dataset

kor <- recount3::create_rse_manual(
    project = "SRP133891",
    project_home = "data_sources/sra",
    organism = "human",
    annotation = "gencode_v26",
    type = "gene",
    verbose = FALSE
)

korData <- as.data.frame(assay(kor))

```

```{r explore}
dim(korData)
nrow(korData[which(apply(korData, 1, sum) > 0),])  # N. rows with sum > 0
nrow(korData[which(apply(korData, 1, sum) > 100),])  # N. rows with sum > 100
nrow(korData[which(apply(korData, 1, sum) > 1000),])  # N. rows with sum > 1000

korColData <- as.data.frame(colData(kor))
# How many samples per condition
table(korColData["sra.experiment_title"])

# Check for NAs
any(is.na(korData))

# Metadata table
korTable <- read.csv(file = "korTable.csv", header = 1, stringsAsFactors = TRUE, row.names = 1)
korTable <- korTable[colnames(korData),] # sort korTable rows to be in the same order as korData columns
```



# Pre-processing (week 2)

__Filtering__ of low expressed genes. 

__Normalization__ of the count matrix: needed to compare different genes in the same sample or the same gene in different samples. The nuber of reads should be normalized with respect to the length of the transcript, otherwse longer genes will for sure have a nigher number of reads associated. 

__Whithin-sample normalization methods__:

Used for comparisons between samples of the same group.

* RPM/CPM: reads/counts per million mapped reads --> correction for sequencing depth
* RPKM/FPKM: correction for sequencing depth and gene length
* TPM: transcripts per million --> correction for sequencing depth and gene length

__Between-sample normalization methods__:

The goal is to identify when the difference between the average counts of two groups is significantly different ( _Differential Expression Analysis_ ). Used when we want to compare gene expression between different samples. The method is a Ttest, uses the standard deviation. 

* DeSeq2: correction for sequencing depth and RNA composition
* TMM: correction for sequencing depth and RNA composition
* GeTMM: correction for sequencing depth and RNA composition and gene length

__Scaling/Log transformation__: normalization to a median of zero by computing the means and subtracting them to the values. Now the boxplots are aligned. Scale normalization is now needed to scale the boxes of the plot, to do this divide the data by the absolute deviation or the mad. In R, this can be done with `scale(x)`. This function automatically does Median centering and Scale normalization. 


```{r filter}
# Filter low expressed genes:
# Only keep genes with at least one count in one dataset in korClean
korClean <- korData[which(apply(korData, 1, sum) > 0),]
dim(korClean)

boxplot(korClean, main = "Boxplot", xlab = "Samples", ylab = "Counts",
        xaxt = "n", col = my_colors[7])
```


```{r normalization}
# Within-sample normalization
korClean2 <- log1p(korData)
# boxplot(korClean2)
korClean3 <- as.data.frame(scale(korClean2))
boxplot(korClean3, main = "Boxplot of normalized counts", xlab = "Samples", ylab = "Counts",
        xaxt = "n", col = my_colors[7])

# To consider for a possible better filtering:
# BiocManager::install("genefilter"); library("genefilter");
# ffun <- filterfun(pOverA(0.20,0.0))
# t.fil <- genefilter(e.mat,ffun)
# small.eset <- log2(e.mat[t.fil,]) 
```



# Differential Expression Analysis (week 3)

Differentially expressed genes can be identified with a _T-test_. Doing a T-test for each gene could cause some problems: multiplicity problems. For this reason, the P-value should be adjusted (Bonferroni or Benjamini-Hochberg correction). The T-test is not ideal for class discovery and prediction but can be used to draw some nice heatmaps or to classify/rank the genes (find the most interesting). If a T-test is needed, it has to be performed on normalized data. 

```{r DEanalysis}
# DESeq2 requires the raw counts matrix (korData)
# and the dataset of the "coldata" (korTable)
# all(colnames(korData)==rownames(korTable)) # to check that names are in the right order

dds <- DESeqDataSetFromMatrix(countData = korClean,
                              colData = korTable,
                              design = ~ Group)
dds
# boxplot(assay(dds), main = "Boxplot of normalized counts", xlab = "Samples", ylab = "Counts",
#         xaxt = "n", col = my_colors[7])

vsd <- vst(dds, blind = FALSE)
# boxplot(assay(vsd), main = "Boxplot of normalized counts", xlab = "Samples", ylab = "Counts",
#         xaxt = "n", col = my_colors[7])

dds <- DESeq(vsd)

```


# Principal Component Analysis (week 3)

It is a method for dimensionality reduction. Thousands of dimensions can be reduced up to 2 or 3 dimensions. The PCA scores show the coordinates with respect to these new dimensions for the samples of the dataset. 



***

17/03



```{r pca}

pca <- prcomp(t(korClean3))

# summary(pca)
# screeplot(pca)

pcaVar <- get_eig(pca)
pcaVar <- pcaVar$variance.percent[1:10]
screeDf <- data.frame("Dimensions" = as.factor(seq(1,10)),
                      "Percentages" = pcaVar,
                      "Labels" = paste(round(pcaVar, 2), "%"))

p <- ggplot(data = screeDf, aes(x=Dimensions, y=Percentages))+
  geom_bar(stat = "identity", fill = my_colors[7])+
  geom_text(aes(label=Labels), vjust=-0.5, color="black", size=3.6)+
  ggtitle("Scree Plot")+
  ylab("Percentge of variance explained")+
  scale_x_discrete(labels = as.factor(seq(1,10)))
p

```

```{r pca_plot, fig.show="hold", out.width="50%"}

## PC 1 and 2

pc12 <- merge(pca$x[,c(1,2)], korTable, by="row.names")

p <- ggplot(pc12, aes(x=PC1, y=PC2, shape=Sex, color=Group)) +
  geom_point(size = 3)+
  scale_color_manual(values=my_colors[c(3,5)])+
  ggtitle("PCA for components 1 and 2")
  
p

## Pc 2 and 3

pc23 <- merge(pca$x[,c(2,3)], korTable, by="row.names")

p <- ggplot(pc23, aes(x=PC2, y=PC3, shape=Sex, color=Group)) +
  geom_point(size = 3)+
  scale_color_manual(values=my_colors[c(3,5)])+
  ggtitle("PCA for components 2 and 3")
  
p

## PC 1 and 3

pc13 <- merge(pca$x[,c(1,3)], korTable, by="row.names")

p <- ggplot(pc13, aes(x=PC1, y=PC3, shape=Sex, color=Group)) +
  geom_point(size = 3)+
  scale_color_manual(values=my_colors[c(3,5)])+
  ggtitle("PCA for components 1 and 3")
  
p


## female

pc12f <- pc12[which(pc12$Sex == "female"),]

p <- ggplot(pc12f, aes(x=PC1, y=PC2, color=Group, size=Age)) +
  geom_point()+
  scale_color_manual(values=my_colors[c(3,5)])+
  ggtitle("PCA for components 1 and 2 - WOMEN")
  
p

## male

pc12m <- pc12[which(pc12$Sex == "male"),]

p <- ggplot(pc12m, aes(x=PC1, y=PC2, color=Group, size=Age)) +
  geom_point(shape=17)+
  scale_color_manual(values=my_colors[c(3,5)])+
  ggtitle("PCA for components 1 and 2 - MEN")
  
p


```


***
# Week 4

22/03

## Classification methods

* Unsupervised approaches: k-means, hierarchical clustering, ...
* Supervised approaches: linear and quadratic discriminants, KNN, decision trees, neural networks, support vector machines, ...

The best method must be evaluated depending on the data.

__Machine learning__ is the study of methods that can learn from and make predictions on data.


### Unsupervised learning

The data have no target attribute. Unsupervidsed learning algorithms are aimed at __data clustering__.

Main aspect of clustering:

* The algorithm (partitional, hierarchical, ...)
* The distance function
* The clustering quality (inter-cluster distance maximized/minimized)

What is a __outlier__: Tukey's criterion. 

`IQR = Q3 - Q1`

Below this threshold: `Q1 - 1.5*IQR`

Above this threshold: `Q3 + 1.5*IQR`

Distances can be computed in a number of ways. One example is the __Euclidean distance__ --> the most inutuitive way to compute distance: with the pythagorean theorem. Another one is the __Manhattan distance__. 

Distances between clusters:
* Simple linkage --> distance between the two closest points in the clusters
* Average linkage (usually the default option)
* Complete linkage --> the two most distant points in the clusters

### K-means

1. Randomly choose _k_ data points (k = number of clusters) to be the initial _centroids_. The number of clusters must be decided by the user, in some cases an algorithm is needed for this, in other we could know it in advance for some reason.
2. Assign each data point to the closest centroid
3. Re-compute the centroids using the current cluster membership.
4. Repeat from point 2 until convergence is reached. (e.g. until centroids remain the same for a couple of repetitions)


Strengths of K-means:
* Simple to understand and implement
* Efficient: time complexity = O(tkn) 
*
*

Limitations of K-means
* K must be chosen, not always convenient.
* Numerical datasets are better because it relies on a numerical distance.
* Greatly affected by outliers: they shift the position of the centroid.
* Sensitive to the initial seeds.
* Only works with clusters with simple shapes, such as ellipsoids of some kind.


### Hierarchical clustering

It produces a __Dendrogram__. Compute a distance matrix and cluster together the points with the lower distance. Then recompute the distance matrix considering the new cluster as one point and continue. There is a finite number of steps. In the end, decide where to cut the tree to have the preferred number of clusters --> the number of clusters is decided at the end.


***

24/03

```{r k-means}

## 4 clusters

km <- kmeans(t(korClean3), 4)
# table(km$cluster)

km2plot <- data.frame("Row.names" = names(km$cluster),
                      "Cluster" = as.factor(km$cluster))
km2plot <- merge(km2plot, pc12, by="Row.names")

p <- ggplot(km2plot, aes(x=PC1, y=PC2, color=Cluster, shape=Group)) +
  geom_point(size = 3)+
  scale_shape_manual(values = c(0,15))+
  scale_color_manual(values = my_colors[c(2,4,6,7)])+
  ggtitle("K-means results - 4 clusters")
  
p


## 2 clusters

km2 <- kmeans(t(korClean3), 2)
# table(km$cluster)

km2plot <- data.frame("Row.names" = names(km2$cluster),
                      "Cluster" = as.factor(km2$cluster))
km2plot <- merge(km2plot, pc12, by="Row.names")

p <- ggplot(km2plot, aes(x=PC1, y=PC2, color=Cluster, shape=Group)) +
  geom_point(size = 3)+
  scale_shape_manual(values = c(0,15))+
  scale_color_manual(values = my_colors[c(6,7)])+
  ggtitle("K-means results - 2 clusters")
  
p

```


```{r hierarchical}

dist_matrix <- dist(t(korClean3), method = "euclidean")
hc <- hclust(dist_matrix, method ="average")
hc$height <- hc$height-80
hclusters <- cutree(hc, k = 7)
hclusters <- hclusters[order(names(hclusters))]
tipData <- data.frame(samples=rownames(korTable),
                      group=korTable$Group,
                      sex=korTable$Sex)

# hc <- ggtree(hc)
hcc <- groupOTU(.data = ggtree(hc), group_name = "cluster", .node = names(hclusters)[which(hclusters==1) ])+
  aes(linetype=cluster)

hcc %<+% tipData +
  layout_dendrogram() +
  geom_tippoint(aes(color=group, shape=sex), size=2.3) +
  scale_color_manual(values = my_colors[c(3,5)]) +
  ggtitle("Hierarchical clustering")
  # geom_tiplab(angle=90, hjust=-0.5, offset=-10, show.legend=FALSE, size=2.8)

```


***
# Week 5

29/03

## Decision trees

__Decision tree learning__ --> supervised method. The tree is a collection of tests: each node is a set, each branch is a feature (a possible answer to the set) and each leaf is a classification. The order of the features in the tree is decided based on entropy: choose the test that brings you clodser to a decision.

When building the tree, the best attribute for each test are chosen with the concept of entropy: compute the gain and choose the possibility that allows to reach the higher information gain. The root attribute is also chosen this way.

Problem: overfitting. Increasing the number of levels, the quality of the training data goes up but the accuracy of the predicion on the test data does not.

To avoid overfitting:

* Prepruning --> stop growing the tree when you think there is enough data to make reliable choices.
* Postpruning --> grow the whole tree and eliminate the nodes that do not have sufficient evidence (most popular)

Other methods are possible (es: cross-validation)

With numerical (continuous) values --> the easiest thing to do is to use a threshold to discretize the numerical data. 

Attributes with many values are a situation that can causes problems, because the gain is going to select it. With a great number of possible values, it is likely that this variable will discriminate well on a small sample. Entropy appreaches the max value. Use _GainRatio_ instead of just _Gain_.

Unknown attribute values --> If node n does not have attribute A: if n tests A, assign to it the most common value of A among the other nodes.

__Gini index__: can be interpreted as expected error rate.

Advantages of decision trees:

* Very fast
* Flexible
* interpretability: it easy to understand the process that led the algorithm to that output

Disadvatages:

* Instability: there is high variance
* Not always competitive with other algorithms in terms of accuracy

__Ensemble learning__ --> Generate a group of base-learners which when combined have higher accuracy.

Each learner uses different:

* Algorithms
* Parameters
* Representations (Modalities)
* Training sets
* Subproblems

Ensamble learning decides classification based on majority voting (??).

__Bootstrap__ --> randomly draw datasets with replacement from the training data, each sample the same size as the original training set.

__Bagging__ --> Bootstrap + Aggregation (voting):

* Take K bootstrap samples (with replacement)
* Train K different classifiers on these bootstrap samples
* For a new query, let all classifiers make a prediction and take an average (or majority vote)

__Random forest classifier__: --> Bagging with decorrelated trees.

* Draw K bootstrap samples of a fixed size
* Grow a DT, randomly sampling a few attributes to split on at each internal node
* Take majority vote using the predictions of the trees for a new query (or take average of predictions)

```{r forest}
# The CARET package is used to divide the dataset in training and test set
set.seed(1)

inTrain <- createDataPartition(korTable$Group, p=0.5)
inTrain <- rownames(korTable)[inTrain$Resample1]

korTrain <- korClean3[inTrain]
# dim(korTrain)
korTest <- korClean3[-which(colnames(korClean3) %in% inTrain)]
# dim(korTest)
yTrain <- korTable[inTrain,"Group"]
yTest <- korTable[-which(colnames(korClean3) %in% inTrain),"Group"]
```


```{r forest2}
# Build the random forest
set.seed(1)
rf <- randomForest(x=t(korTrain), y=yTrain, ntree=500, importance = TRUE)
# Apply on the test set
yhat_rf <- predict(rf, t(korTest))
# Accuracy
mean(yhat_rf==yTest)
# Confusion matrix
table(yhat_rf, yTest)

# Importance
plot(rf)
varImpPlot(rf, sort = TRUE, n.var = 15,
           main = "Importance of feature in random forest classification")
imp <- as.data.frame(importance(rf))

```


```{r heatmap}

imp25 <- rownames(imp)[order(imp$MeanDecreaseGini, decreasing = TRUE)][1:25]
imp25_data <- korClean3[is.element(rownames(korClean3), imp25),]

div <- c("#2e005d", "#662a71", "#945789", "#bc87a5", "#debbc8", 
        "#fcf1f3", "#ffd2d8", "#ffb2b4", "#ff9486", "#ff7852", "#ff6200")
# hmcol <- rev(colorRampPalette(brewer.pal(11,"RdBu"))(256))
hmcol <- viridis(1000, option="magma")
# colnames(imp25_data) <- korTable$Group # This will label the heatmap columns
csc <- rep(my_colors[3],ncol(imp25_data))
csc[korTable$Group=='tumor'] <- my_colors[5]
# column side color will be purple for T and orange for B
imp25_data <- as.matrix(imp25_data)
heatmap(imp25_data, scale="row", col=hmcol, ColSideColors=csc,
        main = "25 most important genes")
legend(x=0.72, y=1, legend=c("normal", "tumor"), fill=my_colors[c(3,5)])

```


```{r}

```

***
# Week 6

05/04

## Linear Methods?

Useful information:

* Between-class distance --> between the centroids of different classes
* Within-class distance --> accumulated distance from the instances to centroid of the class

### Linear Discriminant Analysis

The idea of LDA is to reduce dimensionality while preserving the ability to discriminate. The samples are projected on a line. Choose the line that maximizes the separation of the points.

Find the mathematical centroid: it is the sample mean (mu). 
mu~ means ??? --> projection of the points on the line?
s~2 is the sum of the distances of the projections to the center of the separation line ???

### Performance evaluation of classifiers

Confusion matrix


***
# Week 7

14/04

## Lasso and Ridge

```{r lasso}

# Library "glmnet" is popular to do LASSO regression.

```



***
# Week 8

21/04

## Scudo

The goal of this method is to avoid batch effects and obtain a method that is repeatable and reproducible.

This method compares signatures of different individuals. Each signature is sorted for the expression value. The most important genes for each signature are the most and the less expressed. Then signatures are compared and a map is constructed, where clusters can be seen if they are colsely connected.

Closely connected samples = similar signatures

The idea of ranking makes the method robust to expression variations across samples

Distance measure: similar to GSEA --> the signature (short) is compared to a long ranked list of genes. A running sum is computed and plotted. From the plot it is possible to see where the peak falls: to which part of the long list the signature looks like (?).

Parameters:

* n1 = n2 (usually equal number) --> most and less expressed genes
* N --> percentage of similarity needed to draw an edge between two samples

Supervised version: performes a feature selection before computing the connectivity scores. A new parameter is introduced:

* p --> p-value threshold to consider a feature

Possible improvements:

* better distance calculation method
* ...


```{r scudo}

# The CARET package is used to divide the dataset in training and test set

inTrain <- createDataPartition(korTable$Group, p=0.75)
inTrain <- rownames(korTable)[inTrain$Resample1]

korTrain <- korClean3[inTrain]
# dim(korTrain)
korTest <- korClean3[-which(colnames(korClean3) %in% inTrain)]
# dim(korTest)

# Apply the SCUDO on the training set

trainRes <- scudoTrain(korTrain, groups = korTable[inTrain,]$Group,
                       nTop = 25, nBottom = 25, alpha = 0.05)
trainRes

# inspect signatures 
# upSignatures(trainRes)
# consensusUpSignatures(trainRes)

# generate and plot map of training samples
trainNet <- scudoNetwork(trainRes, N = 0.2)
scudoPlot(trainNet, vertex.label = NA)

# perform validation using testing samples
testRes <- scudoTest(trainRes, korTest,
                     korTable[-which(rownames(korTable)%in%inTrain),]$Group,
                     nTop = 25, nBottom = 25)
testNet <- scudoNetwork(testRes, N = 0.2)
scudoPlot(testNet, vertex.label = NA)

# identify clusters on map
# testClust <- igraph::cluster_spinglass(testNet, spins = 2)
# plot(testClust, testNet, vertex.label = NA)
```

***
# Week 9

26/04

## Functional enrichment analysis

### Gene ontology

This consortium produced three ontologies: the cellular components (CC), molecular functions (MF) and biological processes (BP) ontologies. Ontologies can be considered as some sort of dictionary. The GOs are structured as a hierarchical graph, with the more generic terms at the top and the more specific ones at the bottom. Genes are annotated with a lot of information, divided in levels that are more and more specific of the single gene. Annotations are based on literature and additional inferences computed by the consortium.

Hypergeometric distribution --> for the P-value

***

# Week 10

## Networks theory

***

# Week 11

10/05

## Biological networks

```{r}
geneList <- rownames(imp)[order(imp$MeanDecreaseGini, decreasing = TRUE)][1:100]
geneList <- unlist(strsplit(geneList, ".", fixed = TRUE))
geneList <- geneList[which(startsWith(geneList, "ENS"))]
# For STRING
write.table(geneList, "genesList.txt", quote = FALSE, sep = "\n", row.names = FALSE, col.names = FALSE)
```




